{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:18.501614Z",
     "start_time": "2021-09-20T08:34:18.482575Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f38f5bd9820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if str(type(sys.stdout)) == \"<class 'ipykernel.iostream.OutStream'>\":\n",
    "    saved_std = sys.stdout\n",
    "else:\n",
    "    sys.stdout = saved_std\n",
    "\n",
    "sys.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "1. Cleanup imports\n",
    "1. Decide on how to impute values for each column\n",
    "1. Use OHE and binarize the data.\n",
    "1. Make pipelines that process the data\n",
    "1. Use pipelines to fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.264446Z",
     "start_time": "2021-09-20T08:34:18.505145Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Binarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier,ExtraTreesClassifier)\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:50:59.519698Z",
     "start_time": "2021-09-20T08:50:59.496383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Deck</th>\n",
       "      <th>hasNChildren</th>\n",
       "      <th>hasCabin</th>\n",
       "      <th>has1or2sib</th>\n",
       "      <th>has3PSib</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss.</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked  \\\n",
       "PassengerId                                                                   \n",
       "1                   0       3    male  22.0      1      0   7.2500        S   \n",
       "2                   1       1  female  38.0      1      0  71.2833        C   \n",
       "3                   1       3  female  26.0      0      0   7.9250        S   \n",
       "4                   1       1  female  35.0      1      0  53.1000        S   \n",
       "5                   0       3    male  35.0      0      0   8.0500        S   \n",
       "...               ...     ...     ...   ...    ...    ...      ...      ...   \n",
       "887                 0       2    male  27.0      0      0  13.0000        S   \n",
       "888                 1       1  female  19.0      0      0  30.0000        S   \n",
       "889                 0       3  female  17.0      1      2  23.4500        S   \n",
       "890                 1       1    male  26.0      0      0  30.0000        C   \n",
       "891                 0       3    male  32.0      0      0   7.7500        Q   \n",
       "\n",
       "             Title Deck  hasNChildren  hasCabin  has1or2sib  has3PSib  \n",
       "PassengerId                                                            \n",
       "1              Mr.    U             0         0           1         0  \n",
       "2             Mrs.    C             0         1           1         0  \n",
       "3            Miss.    U             0         0           0         0  \n",
       "4             Mrs.    C             0         1           1         0  \n",
       "5              Mr.    U             0         0           0         0  \n",
       "...            ...  ...           ...       ...         ...       ...  \n",
       "887            Mr.    U             0         0           0         0  \n",
       "888          Miss.    B             0         1           0         0  \n",
       "889          Miss.    U             0         0           1         0  \n",
       "890            Mr.    C             0         1           0         0  \n",
       "891            Mr.    U             0         0           0         0  \n",
       "\n",
       "[891 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('Data/modified.pickle')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical_Pre = make_pipeline(SimpleImputer(strategy='mean'))\n",
    "OneHot_Pre = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "Col_Num = ['Age','Fare','SibSp','Parch']\n",
    "Col_OHE = ['Sex','Deck','Title','Embarked','Pclass']\n",
    "\n",
    "\n",
    "Preprocess = ColumnTransformer([('Col_OHE', OneHot_Pre, Col_OHE),\n",
    "                                ('Col_Num', Numerical_Pre, Col_Num)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreProOHE = [sorted([str(Col) + \"_\" + str(i) for i in df[Col].unique() if str(i) != 'nan']) for Col in Col_OHE]\n",
    "Cols = PreProOHE + Col_Num\n",
    "Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.577862Z",
     "start_time": "2021-09-20T08:34:20.266547Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136/489334966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PassengerId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ticket'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Cabin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv', index_col='PassengerId')\n",
    "y = data['Survived']\n",
    "data.drop(['Name','Ticket','Cabin','Survived'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.583084Z",
     "start_time": "2021-09-20T08:34:20.583071Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', index_col='PassengerId')\n",
    "test.drop(['Name','Cabin','Ticket'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.584514Z",
     "start_time": "2021-09-20T08:34:20.584487Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Categorical_cols = ['Pclass','Embarked','Sex','SibSp']\n",
    "Numerical_cols = ['Fare','Age']\n",
    "\n",
    "Ctgrcl_trns = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder())\n",
    "\n",
    "# Sex: ['male','female','male','other']\n",
    "# OHE: [[1,0,1,0],[0,1,0,0],[0,0,0,1]]\n",
    "\n",
    "Nmrcl_trns = make_pipeline(SimpleImputer(strategy='mean'))\n",
    "Preprocess = ColumnTransformer([('Categorical_cols', Ctgrcl_trns,Categorical_cols),\n",
    "                                ('Numrecial_cols',Nmrcl_trns,Numerical_cols)])\n",
    "\n",
    "Gradient = GradientBoostingClassifier(warm_start=True)\n",
    "Pipe_Grad = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('Gradient', Gradient)\n",
    "])\n",
    "\n",
    "Forest = RandomForestClassifier(warm_start=True)\n",
    "Pipe_Forest = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('Forest', Forest)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.588207Z",
     "start_time": "2021-09-20T08:34:20.588182Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "params_Frst = dict(Forest__n_estimators=[50,100,200,300,500],\n",
    "                   Forest__max_depth=[7,8,9,None],\n",
    "                   Forest__random_state=[11])\n",
    "\n",
    "params_Grad = dict(Gradient__n_estimators = [100,200,300,400,500],\n",
    "                   Gradient__max_depth=range(1,4), \n",
    "                   Gradient__random_state=[12],\n",
    "                   Gradient__learning_rate=[0.1,0.05,0.01])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.589967Z",
     "start_time": "2021-09-20T08:34:20.589951Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def print_best(Grid,Prints = True):\n",
    "    if Prints:\n",
    "        print(Grid.best_score_,Grid.best_params_)\n",
    "    if not Prints:\n",
    "        return [Grid.best_score_,Grid.best_params_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_Grid_NR = GridSearchCV(Pipe_Forest,param_grid=params_Frst,scoring='f1',cv=10,return_train_score=True)\n",
    "\n",
    "Grad_Grid_NR = GridSearchCV(Pipe_Grad,param_grid=params_Grad,scoring='roc_auc',cv=10,return_train_score=True)\n",
    "\n",
    "Grad_Grid_NR.fit(data,y)\n",
    "print_best(Grad_Grid_NR)\n",
    "\n",
    "Forest_Grid_NR.fit(data,y)\n",
    "print_best(Forest_Grid_NR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_Grid_OF = GridSearchCV(Pipe_Forest,param_grid=params_Frst,scoring='accuracy',cv=5,return_train_score=True)\n",
    "Grad_Grid_OF = GridSearchCV(Pipe_Grad,param_grid=params_Grad,scoring='accuracy',cv=5,return_train_score=True)\n",
    "\n",
    "Grad_Grid_OF.fit(data,y)\n",
    "print_best(Grad_Grid_OF)\n",
    "\n",
    "Forest_Grid_OF.fit(data,y)\n",
    "print_best(Forest_Grid_OF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_pred_OF = pd.Series(Forest_Grid_OF.predict(test), index=test.index, name='Survived')\n",
    "Grad_pred_OF = pd.Series(Grad_Grid_OF.predict(test), index=test.index, name='Survived')\n",
    "\n",
    "print(metrics.confusion_matrix(y,Forest_Grid_OF.predict(data)))\n",
    "print(metrics.confusion_matrix(y,Grad_Grid_OF.predict(data)))\n",
    "\n",
    "\n",
    "Forest_pred_OF.to_csv('Predictions/Forest_Pred_OF_1.csv')\n",
    "Grad_pred_OF.to_csv('Predictions/Grad_Pred_OF_1.csv')\n",
    "\n",
    "\n",
    "# Confusion Matrix:\n",
    "#    | 0  | 1  |\n",
    "#  0 | TN | FN |\n",
    "#  1 | FP | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_pred_NR = pd.Series(Forest_Grid_NR.predict(test), index=test.index, name='Survived')\n",
    "Grad_pred_NR = pd.Series(Grad_Grid_NR.predict(test), index=test.index, name='Survived')\n",
    "\n",
    "print(metrics.confusion_matrix(y,Forest_Grid_NR.predict(data)))\n",
    "print(metrics.confusion_matrix(y,Grad_Grid_NR.predict(data)))\n",
    "\n",
    "Forest_pred_NR.to_csv('Predictions/Forest_Pred_NR_1.csv')\n",
    "Grad_pred_NR.to_csv('Predictions/Grad_Pred_NR_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.591579Z",
     "start_time": "2021-09-20T08:34:20.591564Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "params_Grad_f1 = dict(Gradient__n_estimators = [100,400,600],\n",
    "                   Gradient__max_depth=[2,5], \n",
    "                   Gradient__random_state=[42],\n",
    "                   Gradient__learning_rate=[0.2,0.1,0.05])\n",
    "\n",
    "# params_Grad_f1 = dict(Gradient__n_estimators = [400],\n",
    "#                    Gradient__max_depth=[2], \n",
    "#                    Gradient__random_state=[42],\n",
    "#                    Gradient__learning_rate=[0.1])\n",
    "\n",
    "params_Frst_f1 = dict(Forest__n_estimators=[200,500,700],\n",
    "                   Forest__max_depth=[None,6,9,12],\n",
    "                   Forest__random_state=[42])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591260150442718 {'Gradient__learning_rate': 0.1, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7551045173645473 {'Forest__max_depth': 12, 'Forest__n_estimators': 700, 'Forest__random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Grad_Grid_f1 = GridSearchCV(Pipe_Grad,param_grid=params_Grad_f1,scoring='f1',cv=5,verbose=3,n_jobs=1)\n",
    "Forest_Grid_f1 = GridSearchCV(Pipe_Forest,param_grid=params_Frst_f1,scoring='f1',cv=5,verbose=3,n_jobs=1)\n",
    "\n",
    "sys.stdout = open(1, 'w')\n",
    "Grad_Grid_f1.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(Grad_Grid_f1)\n",
    "\n",
    "sys.stdout = open(1, 'w')\n",
    "Forest_Grid_f1.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(Forest_Grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[517  32]\n",
      " [ 57 285]]\n",
      "[[543   6]\n",
      " [ 27 315]]\n"
     ]
    }
   ],
   "source": [
    "Grad_pred_f1 = pd.Series(Grad_Grid_f1.predict(test), index=test.index, name='Survived')\n",
    "Forest_pred_f1 = pd.Series(Forest_Grid_f1.predict(test), index=test.index, name='Survived')\n",
    "\n",
    "print(metrics.confusion_matrix(y,Grad_Grid_f1.predict(data)))\n",
    "print(metrics.confusion_matrix(y,Forest_Grid_f1.predict(data)))\n",
    "\n",
    "\n",
    "Grad_pred_f1.to_csv('Predictions/Grad_Pred_f1.csv')\n",
    "Forest_pred_f1.to_csv('Predictions/Forest_Pred_f1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.593968Z",
     "start_time": "2021-09-20T08:34:20.593946Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Tree = tree.DecisionTreeClassifier(random_state=32)\n",
    "Pipe_Tree = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('Tree', Tree)\n",
    "])\n",
    "\n",
    "params_Tree = dict(Tree__max_depth = [None],\n",
    "                   Tree__splitter = ['best','random'],\n",
    "                   Tree__min_samples_split = range(2,4),\n",
    "                   Tree__min_samples_leaf = range(1,4),\n",
    "                   Tree__max_leaf_nodes = [None] + list(range(150,300,50))\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7534598724095104 {'Tree__max_depth': None, 'Tree__max_leaf_nodes': 200, 'Tree__min_samples_leaf': 2, 'Tree__min_samples_split': 2, 'Tree__splitter': 'random'}\n",
      "[[521  28]\n",
      " [ 76 266]]\n"
     ]
    }
   ],
   "source": [
    "Tree_Grid = GridSearchCV(Pipe_Tree,param_grid=params_Tree,scoring='f1',cv=10,verbose=3)\n",
    "\n",
    "sys.stdout = open(1, 'w')\n",
    "Tree_Grid.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(Tree_Grid)\n",
    "print(metrics.confusion_matrix(y,Tree_Grid.predict(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.595471Z",
     "start_time": "2021-09-20T08:34:20.595456Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=32)\n",
    "Pipe_logreg = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "params_logreg = dict(logreg__penalty = ['l1','l2'],\n",
    "                     logreg__solver = ['liblinear'],\n",
    "                     logreg__max_iter = [100,125,75],\n",
    "                     logreg__C = np.arange(1,2,0.05),\n",
    "                     logreg__class_weight = ['balanced',None]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734546590750368 {'logreg__C': 1.1, 'logreg__class_weight': 'balanced', 'logreg__max_iter': 100, 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}\n",
      "[[437 112]\n",
      " [ 73 269]]\n"
     ]
    }
   ],
   "source": [
    "logreg_Grid = GridSearchCV(Pipe_logreg,param_grid=params_logreg,scoring='f1',cv=10,verbose=3)\n",
    "\n",
    "sys.stdout = open(1, 'w')\n",
    "logreg_Grid.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(logreg_Grid)\n",
    "print(metrics.confusion_matrix(y,logreg_Grid.predict(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.596629Z",
     "start_time": "2021-09-20T08:34:20.596615Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Extra_For = ExtraTreesClassifier(warm_start=True,random_state=1)\n",
    "Pipe_Extra = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('Extra', Extra_For)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.598224Z",
     "start_time": "2021-09-20T08:34:20.598200Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "params_Extra = dict(Extra__n_estimators = [50,100,200],\n",
    "                    Extra__max_depth = [None],\n",
    "                    Extra__class_weight = [None],\n",
    "                    Extra__ccp_alpha = [0.0]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7321402121164768 {'Extra__ccp_alpha': 0.0, 'Extra__class_weight': None, 'Extra__max_depth': None, 'Extra__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "sys.stdout = open(1, 'w')\n",
    "Extra_Grid_f1 = GridSearchCV(Pipe_Extra,param_grid=params_Extra,scoring='f1',cv=10,n_jobs=1,verbose=3)\n",
    "Extra_Grid_f1.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(Extra_Grid_f1)\n",
    "print(metrics.confusion_matrix(y,Extra_Grid_f1.predict(data)))\n",
    "pd.Series(Extra_Grid_f1.predict(test), index=test.index, name='Survived').to_csv('Predictions/Extra_Pred_f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.600230Z",
     "start_time": "2021-09-20T08:34:20.600204Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Ada = AdaBoostClassifier(random_state=1,base_estimator = tree.DecisionTreeClassifier(random_state=1))\n",
    "Pipe_Ada = Pipeline([\n",
    "    ('Preprocessing', Preprocess),\n",
    "    ('Ada', Ada)\n",
    "])\n",
    "# possible models for Ada:\n",
    "# ExtraTrees\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.605190Z",
     "start_time": "2021-09-20T08:34:20.605154Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "params_Ada = dict(\n",
    "                  Ada__n_estimators = [50,100,200]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744563123023356 {'Ada__n_estimators': 50}\n",
      "[[539  10]\n",
      " [  7 335]]\n"
     ]
    }
   ],
   "source": [
    "sys.stdout = open(1, 'w')\n",
    "Ada_Grid_f1 = GridSearchCV(Pipe_Ada,param_grid=params_Ada,cv=10,verbose=3,scoring='f1')\n",
    "Ada_Grid_f1.fit(data,y)\n",
    "print('Done with grid :)')\n",
    "sys.stdout = saved_std\n",
    "\n",
    "print_best(Ada_Grid_f1)\n",
    "print(metrics.confusion_matrix(y,Ada_Grid_f1.predict(data)))\n",
    "pd.Series(Ada_Grid_f1.predict(test), index=test.index, name='Survived').to_csv('Predictions/Ada_Pred_f1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "init_cell": true
   },
   "source": [
    "## Bests models: \n",
    "1. GradientBoosting (f1)\n",
    "2. RandomForest (f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to feature engineering\n",
    "Our features should include these facts:\n",
    "1. Kids and teens have a higher chance of surviving\n",
    "2. People with family have a higher chance of surviving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.611371Z",
     "start_time": "2021-09-20T08:34:20.611341Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Age'] = Nmrcl_trns.fit_transform(data[['Age']])\n",
    "data['Has_Someone'] = Binarizer(threshold=0.9).transform([data['Parch'] + data['SibSp']])[0]\n",
    "data['Has_SibSp'] = Binarizer(threshold=0.9).transform([data['SibSp']])[0]\n",
    "data['Under_17'] = 1 - Binarizer(threshold=17).transform([(data['Age'])])[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.612569Z",
     "start_time": "2021-09-20T08:34:20.612556Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "test['Age'] = Nmrcl_trns.fit_transform(test[['Age']])\n",
    "test['Has_Someone'] = Binarizer(threshold=0.9).transform([test['Parch'] + test['SibSp']])[0]\n",
    "test['Has_SibSp'] = Binarizer(threshold=0.9).transform([test['SibSp']])[0]\n",
    "test['Under_17'] = 1 - Binarizer(threshold=17).transform([(test['Age'])])[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.614644Z",
     "start_time": "2021-09-20T08:34:20.614611Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Feature_Pairs = [\n",
    "    (['Sex','Pclass','Embarked'],['SibSp','Age','Fare']),\n",
    "    (['Sex','Pclass','Embarked','Under_17','Has_SibSp'],['Fare']),\n",
    "    (['Sex','Pclass','Embarked','Under_17','Has_SibSp'],[]),\n",
    "    (['Sex','Pclass','Embarked','Has_SibSp'],['Age']),\n",
    "    (['Sex','Pclass','Embarked','Under_17','Has_Someone'],[]),\n",
    "    (['Sex','Pclass','SibSp','Embarked'],['Age','Fare']),\n",
    "    (['Sex','Pclass','SibSp','Embarked','Under_17','Has_Someone'],['Age','Fare']),\n",
    "    (['Sex','Pclass','Embarked','Under_17'],['SibSp','Age','Fare']),\n",
    "]\n",
    "\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702513487952218 {'Gradient__learning_rate': 0.05, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 600, 'Gradient__random_state': 42}\n",
      "0.7731677336618445 {'Forest__max_depth': 9, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked'], ['SibSp', 'Age', 'Fare'])\n",
      "0.7732873120586929 {'Gradient__learning_rate': 0.1, 'Gradient__max_depth': 5, 'Gradient__n_estimators': 100, 'Gradient__random_state': 42}\n",
      "0.7556297083572934 {'Forest__max_depth': 9, 'Forest__n_estimators': 500, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Under_17', 'Has_SibSp'], ['Fare'])\n"
     ]
    }
   ],
   "source": [
    "for pair in Feature_Pairs[0:2]:\n",
    "    Preprocess2 = ColumnTransformer([('Categorical_cols', Ctgrcl_trns,pair[0]),\n",
    "                                 ('Numrecial_cols',Nmrcl_trns,pair[1])])\n",
    "    \n",
    "    Grad_Fin = GradientBoostingClassifier(warm_start=True)\n",
    "    Pipe_Grad_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Gradient', Gradient)\n",
    "    ])\n",
    "\n",
    "    Forest_Fin = RandomForestClassifier(warm_start=True)\n",
    "    Pipe_Forest_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Forest', Forest)\n",
    "    ])\n",
    "    \n",
    "    Grad_Grid_Fin = GridSearchCV(Pipe_Grad_Fin,param_grid=params_Grad_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "    Forest_Grid_Fin = GridSearchCV(Pipe_Forest_Fin,param_grid=params_Frst_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Grad_Grid_Fin.fit(data,y)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append([print_best(Grad_Grid_Fin,Prints=False)])\n",
    "    \n",
    "    print_best(Grad_Grid_Fin)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Forest_Grid_Fin.fit(data,y)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append([print_best(Forest_Grid_Fin,Prints=False)])\n",
    "\n",
    "    print_best(Forest_Grid_Fin)\n",
    "    print(\"\\n\",pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7320755603515294 {'Gradient__learning_rate': 0.1, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7216048015283008 {'Forest__max_depth': 6, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Under_17', 'Has_SibSp'], [])\n",
      "0.7549350622890174 {'Gradient__learning_rate': 0.2, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7447045928248592 {'Forest__max_depth': 6, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Has_SibSp'], ['Age'])\n",
      "0.7266765437720577 {'Gradient__learning_rate': 0.05, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7142252496161967 {'Forest__max_depth': None, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Under_17', 'Has_Someone'], [])\n",
      "0.7697356668170433 {'Gradient__learning_rate': 0.1, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7693101636996904 {'Forest__max_depth': 9, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'SibSp', 'Embarked'], ['Age', 'Fare'])\n",
      "0.7688505877991048 {'Gradient__learning_rate': 0.1, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 400, 'Gradient__random_state': 42}\n",
      "0.7600273193307135 {'Forest__max_depth': 9, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'SibSp', 'Embarked', 'Under_17', 'Has_Someone'], ['Age', 'Fare'])\n"
     ]
    }
   ],
   "source": [
    "for pair in Feature_Pairs[2:-1]:\n",
    "    Preprocess2 = ColumnTransformer([('Categorical_cols', Ctgrcl_trns,pair[0]),\n",
    "                                 ('Numrecial_cols',Nmrcl_trns,pair[1])])\n",
    "    \n",
    "    Grad_Fin = GradientBoostingClassifier(warm_start=True)\n",
    "    Pipe_Grad_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Gradient', Gradient)\n",
    "    ])\n",
    "\n",
    "    Forest_Fin = RandomForestClassifier(warm_start=True)\n",
    "    Pipe_Forest_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Forest', Forest)\n",
    "    ])\n",
    "    \n",
    "    Grad_Grid_Fin = GridSearchCV(Pipe_Grad_Fin,param_grid=params_Grad_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "    Forest_Grid_Fin = GridSearchCV(Pipe_Forest_Fin,param_grid=params_Frst_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Grad_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append([print_best(Grad_Grid_Fin,Prints=False)])\n",
    "\n",
    "    print_best(Grad_Grid_Fin)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Forest_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append([print_best(Forest_Grid_Fin,Prints=False)])\n",
    "\n",
    "    print_best(Forest_Grid_Fin)\n",
    "    print(\"\\n\",pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702513487952218 {'Gradient__learning_rate': 0.05, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 600, 'Gradient__random_state': 42}\n",
      "0.7710090271179103 {'Forest__max_depth': 12, 'Forest__n_estimators': 500, 'Forest__random_state': 42}\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Under_17'], ['SibSp', 'Age', 'Fare'])\n"
     ]
    }
   ],
   "source": [
    "for pair in [Feature_Pairs[-1]]:\n",
    "    Preprocess2 = ColumnTransformer([('Categorical_cols', Ctgrcl_trns,pair[0]),\n",
    "                                 ('Numrecial_cols',Nmrcl_trns,pair[1])])\n",
    "    \n",
    "    Grad_Fin = GradientBoostingClassifier(warm_start=True)\n",
    "    Pipe_Grad_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Gradient', Gradient)\n",
    "    ])\n",
    "\n",
    "    Forest_Fin = RandomForestClassifier(warm_start=True)\n",
    "    Pipe_Forest_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Forest', Forest)\n",
    "    ])\n",
    "    \n",
    "    Grad_Grid_Fin = GridSearchCV(Pipe_Grad_Fin,param_grid=params_Grad_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "    Forest_Grid_Fin = GridSearchCV(Pipe_Forest_Fin,param_grid=params_Frst_f1,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Grad_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append(print_best(Grad_Grid_Fin,Prints=False))\n",
    "\n",
    "    print_best(Grad_Grid_Fin)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Forest_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append([print_best(Forest_Grid_Fin,Prints=False)])\n",
    "\n",
    "    print_best(Forest_Grid_Fin)\n",
    "    print(\"\\n\",pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.616244Z",
     "start_time": "2021-09-20T08:34:20.616225Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "Best_Pairs = [(['Sex', 'Pclass', 'Embarked'], ['SibSp', 'Age', 'Fare']),\n",
    "              (['Sex', 'Pclass', 'Embarked'], ['Parch','SibSp', 'Age', 'Fare'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T08:34:20.618042Z",
     "start_time": "2021-09-20T08:34:20.618010Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "params_Grad_Fin = [\n",
    "    dict(Gradient__n_estimators = [400,600,700],\n",
    "         Gradient__max_depth=[2,3], \n",
    "         Gradient__random_state=[42],\n",
    "         Gradient__learning_rate=[0.1,0.05]),\n",
    "    dict(Gradient__n_estimators = [100,200],\n",
    "         Gradient__max_depth=[8,9,10,12], \n",
    "         Gradient__random_state=[42],\n",
    "         Gradient__learning_rate=[0.1,0.05])\n",
    "                  ]\n",
    "\n",
    "\n",
    "params_Frst_Fin = dict(Forest__n_estimators=[200,350,500],\n",
    "                   Forest__max_depth=[None,8,9,10],\n",
    "                   Forest__random_state=[42])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702513487952218 {'Gradient__learning_rate': 0.05, 'Gradient__max_depth': 2, 'Gradient__n_estimators': 600, 'Gradient__random_state': 42}\n",
      "[[519  30]\n",
      " [ 66 276]]\n",
      "0.7731677336618445 {'Forest__max_depth': 9, 'Forest__n_estimators': 200, 'Forest__random_state': 42}\n",
      "[[538  11]\n",
      " [ 44 298]]\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked'], ['SibSp', 'Age', 'Fare'])\n",
      "0.7603701395235238 {'Gradient__learning_rate': 0.05, 'Gradient__max_depth': 9, 'Gradient__n_estimators': 100, 'Gradient__random_state': 42}\n",
      "[[530  19]\n",
      " [ 49 293]]\n",
      "0.7608696728957778 {'Forest__max_depth': 8, 'Forest__n_estimators': 500, 'Forest__random_state': 42}\n",
      "[[527  22]\n",
      " [ 71 271]]\n",
      "\n",
      " (['Sex', 'Pclass', 'Embarked', 'Under_17', 'Has_SibSp'], ['Fare'])\n"
     ]
    }
   ],
   "source": [
    "for index,pair in enumerate(Best_Pairs):\n",
    "    Preprocess2 = ColumnTransformer([('Categorical_cols', Ctgrcl_trns,pair[0]),\n",
    "                                 ('Numrecial_cols',Nmrcl_trns,pair[1])])\n",
    "    \n",
    "    Grad_Fin = GradientBoostingClassifier(warm_start=True)\n",
    "    Pipe_Grad_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Gradient', Gradient)\n",
    "    ])\n",
    "\n",
    "    Forest_Fin = RandomForestClassifier(warm_start=True)\n",
    "    Pipe_Forest_Fin = Pipeline([\n",
    "        ('Preprocessing', Preprocess2),\n",
    "        ('Forest', Forest)\n",
    "    ])\n",
    "    \n",
    "    Grad_Grid_Fin = GridSearchCV(Pipe_Grad_Fin,param_grid=params_Grad_Fin[index],scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "    Forest_Grid_Fin = GridSearchCV(Pipe_Forest_Fin,param_grid=params_Frst_Fin,scoring='f1',cv=10,verbose=10,n_jobs=1)\n",
    "\n",
    "    sys.stdout = open(1, 'w')\n",
    "    Grad_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append(print_best(Grad_Grid_Fin,Prints=False))\n",
    "\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print(metrics.confusion_matrix(y,Grad_Grid_Fin.predict(data)))\n",
    "    pd.Series(Grad_Grid_Fin.predict(test), index=test.index, name='Survived'\n",
    "             ).to_csv('Predictions/Best_Pred_Grad_{}.csv'.format(index + 1))\n",
    "    \n",
    "    sys.stdout = open(1, 'w')\n",
    "    Forest_Grid_Fin.fit(data,y)\n",
    "    print_best(Grad_Grid_Fin)\n",
    "    print('Done with grid :)')\n",
    "    sys.stdout = saved_std\n",
    "    scores.append(print_best(Forest_Grid_Fin,Prints=False))\n",
    "\n",
    "    print_best(Forest_Grid_Fin)\n",
    "    print(metrics.confusion_matrix(y,Forest_Grid_Fin.predict(data)))\n",
    "    pd.Series(Forest_Grid_Fin.predict(test), index=test.index, name='Survived'\n",
    "             ).to_csv('Predictions/Best_Pred_Forest_{}.csv'.format(index + 1))\n",
    "    \n",
    "    print(\"\\n\",pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ctgrcl_Best = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(),)\n",
    "Nmrcl_Best = \n",
    "for index,pair in enumerate(Best_Pairs):\n",
    "    Preprocess3 = ColumnTransformer([('Categorical_cols', Ctgrcl_Best,pair[0]),\n",
    "                                 ('Numrecial_cols',Nmrcl_Best,pair[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
